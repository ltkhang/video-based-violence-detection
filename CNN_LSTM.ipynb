{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"CNN _LSTM.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"Uv1jcKJ6IppS"},"source":["import numpy as np\n","import os\n","from time import time\n","import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g95tLNEwIppU"},"source":["from tensorflow.keras.utils import Sequence\n","from tensorflow.keras import utils as np_utils\n","\n","class DataGenerator(Sequence):\n","    \"\"\"Data Generator inherited from keras.utils.Sequence\n","    Args: \n","        directory: the path of data set, and each sub-folder will be assigned to one class\n","        batch_size: the number of data points in each batch\n","        shuffle: whether to shuffle the data per epoch\n","    Note:\n","        If you want to load file with other data format, please fix the method of \"load_data\" as you want\n","    \"\"\"\n","    def __init__(self, directory, batch_size=1, shuffle=True, data_augmentation=True):\n","        # Initialize the params\n","        self.batch_size = batch_size\n","        self.directory = directory\n","        self.shuffle = shuffle\n","        self.data_aug = data_augmentation\n","        # Load all the save_path of files, and create a dictionary that save the pair of \"data:label\"\n","        self.X_path, self.Y_dict = self.search_data() \n","        # Print basic statistics information\n","        self.print_stats()\n","        return None\n","        \n","    def search_data(self):\n","        X_path = []\n","        Y_dict = {}\n","        # list all kinds of sub-folders\n","        self.dirs = sorted(os.listdir(self.directory))\n","        one_hots = np_utils.to_categorical(range(len(self.dirs)))\n","        for i,folder in enumerate(self.dirs):\n","            folder_path = os.path.join(self.directory,folder)\n","            for file in os.listdir(folder_path):\n","                file_path = os.path.join(folder_path,file)\n","                # append the each file path, and keep its label  \n","                X_path.append(file_path)\n","                Y_dict[file_path] = one_hots[i]\n","        return X_path, Y_dict\n","    \n","    def print_stats(self):\n","        # calculate basic information\n","        self.n_files = len(self.X_path)\n","        self.n_classes = len(self.dirs)\n","        self.indexes = np.arange(len(self.X_path))\n","        np.random.shuffle(self.indexes)\n","        # Output states\n","        print(\"Found {} files belonging to {} classes.\".format(self.n_files,self.n_classes))\n","        for i,label in enumerate(self.dirs):\n","            print('%10s : '%(label),i)\n","        return None\n","    \n","    def __len__(self):\n","        # calculate the iterations of each epoch\n","        steps_per_epoch = np.ceil(len(self.X_path) / float(self.batch_size))\n","        return int(steps_per_epoch)\n","\n","    def __getitem__(self, index):\n","        \"\"\"Get the data of each batch\n","        \"\"\"\n","        # get the indexs of each batch\n","        batch_indexs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","        # using batch_indexs to get path of current batch\n","        batch_path = [self.X_path[k] for k in batch_indexs]\n","        # get batch data\n","        batch_x, batch_y = self.data_generation(batch_path)\n","        return batch_x, batch_y\n","\n","    def on_epoch_end(self):\n","        # shuffle the data at each end of epoch\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def data_generation(self, batch_path):\n","        # load data into memory, you can change the np.load to any method you want\n","        batch_x = [self.load_data(x) for x in batch_path]\n","        batch_y = [self.Y_dict[x] for x in batch_path]\n","        # transfer the data format and take one-hot coding for labels\n","        batch_x = np.array(batch_x)\n","        batch_y = np.array(batch_y)\n","        return batch_x, batch_y\n","      \n","    def normalize(self, data):\n","        mean = np.mean(data)\n","        std = np.std(data)\n","        return (data-mean) / std\n","    \n","    def random_flip(self, video, prob):\n","        s = np.random.rand()\n","        if s < prob:\n","            video = np.flip(m=video, axis=2)\n","        return video    \n","    \n","    def uniform_sampling(self, video, target_frames=64):\n","        # get total frames of input video and calculate sampling interval \n","        len_frames = int(len(video))\n","        interval = int(np.ceil(len_frames/target_frames))\n","        # init empty list for sampled video and \n","        sampled_video = []\n","        for i in range(0,len_frames,interval):\n","            sampled_video.append(video[i])     \n","        # calculate numer of padded frames and fix it \n","        num_pad = target_frames - len(sampled_video)\n","        if num_pad>0:\n","            padding = [video[i] for i in range(-num_pad,0)]\n","            sampled_video += padding     \n","        # get sampled video\n","        return np.array(sampled_video, dtype=np.float32)\n","    \n","    def color_jitter(self,video):\n","        # range of s-component: 0-1\n","        # range of v component: 0-255\n","        s_jitter = np.random.uniform(-0.2,0.2)\n","        v_jitter = np.random.uniform(-30,30)\n","        for i in range(len(video)):\n","            hsv = cv2.cvtColor(video[i], cv2.COLOR_RGB2HSV)\n","            s = hsv[...,1] + s_jitter\n","            v = hsv[...,2] + v_jitter\n","            s[s<0] = 0\n","            s[s>1] = 1\n","            v[v<0] = 0\n","            v[v>255] = 255\n","            hsv[...,1] = s\n","            hsv[...,2] = v\n","            video[i] = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n","        return video\n","        \n","    def load_data(self, path):\n","        data = np.load(path, mmap_mode='r')[...,:3]\n","        data = np.float32(data)\n","        # sampling 64 frames uniformly from the entire video\n","        data = self.uniform_sampling(video=data, target_frames=64)\n","        # whether to utilize the data augmentation\n","        if  self.data_aug:\n","            data = self.color_jitter(data)\n","            data = self.random_flip(data, prob=0.5)\n","        # normalize\n","        data = self.normalize(data)\n","        return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppjLmMlAIppV"},"source":["from tensorflow.keras.layers import *\n","from tensorflow.keras import *\n","from tensorflow.keras.applications.mobilenet import MobileNet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y5ZkFlY5IppW"},"source":["import tensorflow.keras.backend as K\n","from tensorflow.keras.callbacks import LearningRateScheduler\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","def scheduler(epoch):\n","    # Every 10 epochs, the learning rate is reduced to 1/10 of the original\n","    if epoch % 10 == 0 and epoch != 0:\n","        lr = K.get_value(model.optimizer.lr)\n","        K.set_value(model.optimizer.lr, lr * 0.5)\n","    return K.get_value(model.optimizer.lr)\n","\n","reduce_lr = LearningRateScheduler(scheduler)\n","model_chk = ModelCheckpoint('cnn_lstm/epoch-{epoch:02d}-val_acc-{val_acc:.2f}.h5', monitor='val_acc')\n","callbacks_list = [reduce_lr, model_chk]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_q08y3Q7IppW","outputId":"c16ad547-a421-451b-e897-77e7e9e2640b"},"source":["backbone = MobileNet(input_shape=(224, 224, 3), alpha=0.25, include_top=False, pooling='max', weights='imagenet')\n","model = Sequential()\n","model.add(InputLayer(input_shape=(64, 224, 224, 3)))\n","model.add(TimeDistributed(backbone))\n","model.add(LSTM(32, return_sequences=True))\n","model.add(LSTM(16))\n","model.add(Dense(2, activation='softmax'))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_2_5_224_tf_no_top.h5\n","2113536/2108140 [==============================] - 0s 0us/step\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","time_distributed (TimeDistri (None, 64, 256)           218544    \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 64, 32)            36992     \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 16)                3136      \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 34        \n","=================================================================\n","Total params: 258,706\n","Trainable params: 253,234\n","Non-trainable params: 5,472\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UDobn_4AIppX"},"source":["from tensorflow.keras.optimizers import Adam, SGD\n","\n","adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n","sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhLbEUbgIppX","outputId":"fa63b75a-1390-4834-c190-dbf525b9d40d"},"source":["batch_size = 4\n","dataset = 'D:/Khang//RWF-npy'\n","\n","train_generator = DataGenerator(directory='{}/train'.format(dataset), \n","                                batch_size=batch_size, \n","                                data_augmentation=True)\n","\n","val_generator = DataGenerator(directory='{}/val'.format(dataset),\n","                              batch_size=1, \n","                              data_augmentation=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 1600 files belonging to 2 classes.\n","     Fight :  0\n","  NonFight :  1\n","Found 400 files belonging to 2 classes.\n","     Fight :  0\n","  NonFight :  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9tOUdQaTIppY"},"source":["num_epochs  = 30\n","\n","model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","hist = model.fit_generator(\n","    generator=train_generator, \n","    validation_data=val_generator,\n","    callbacks=callbacks_list,\n","    epochs=num_epochs,\n","    steps_per_epoch=len(train_generator),\n","    validation_steps=len(val_generator))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OTFkLawUIppY","outputId":"290c2684-181a-46ef-a73b-f1eb2d213d67"},"source":["model.evaluate_generator(val_generator)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-9-3d7f72b93b2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m         callbacks=callbacks)\n\u001b[0m\u001b[0;32m   1364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m   def predict_generator(self,\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1103\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"0FB0GwgpIppY","outputId":"b7632aa8-a014-463b-ac1b-eed074fc3a5a"},"source":["import matplotlib.pyplot as plt\n","history = hist\n","print(history.history.keys())\n","# summarize history for accuracy\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'hist' is not defined","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-10-45602183000d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# summarize history for accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'hist' is not defined"]}]}]}